# Test Scenario C: Combined Pipeline and Tensor Parallelism
# Combined focus on both macro operations and analytical collectives

simulation:
  max_simulation_time: 600  # 10 minutes
  random_seed: 42
  lod: "high"  # Change to "medium" for comparison

model_characteristics_db_path: "./configs/model_params.json"

hardware_profile:
  compute_devices:
    - device_id: "gpu0"
      device_type: "GPU"
      peak_tflops:
        fp16: 312
        int8: 624
      memory_capacity_bytes: 80_000_000_000  # 80 GB
      memory_gbps: 2039  # GB/s
      processing_units: 108  # SMs
    - device_id: "gpu1"
      device_type: "GPU"
      peak_tflops:
        fp16: 312
        int8: 624
      memory_capacity_bytes: 80_000_000_000  # 80 GB
      memory_gbps: 2039  # GB/s
      processing_units: 108  # SMs
    - device_id: "gpu2"
      device_type: "GPU"
      peak_tflops:
        fp16: 312
        int8: 624
      memory_capacity_bytes: 80_000_000_000  # 80 GB
      memory_gbps: 2039  # GB/s
      processing_units: 108  # SMs
    - device_id: "gpu3"
      device_type: "GPU"
      peak_tflops:
        fp16: 312
        int8: 624
      memory_capacity_bytes: 80_000_000_000  # 80 GB
      memory_gbps: 2039  # GB/s
      processing_units: 108  # SMs
    - device_id: "gpu4"
      device_type: "GPU"
      peak_tflops:
        fp16: 312
        int8: 624
      memory_capacity_bytes: 80_000_000_000  # 80 GB
      memory_gbps: 2039  # GB/s
      processing_units: 108  # SMs
    - device_id: "gpu5"
      device_type: "GPU"
      peak_tflops:
        fp16: 312
        int8: 624
      memory_capacity_bytes: 80_000_000_000  # 80 GB
      memory_gbps: 2039  # GB/s
      processing_units: 108  # SMs
    - device_id: "gpu6"
      device_type: "GPU"
      peak_tflops:
        fp16: 312
        int8: 624
      memory_capacity_bytes: 80_000_000_000  # 80 GB
      memory_gbps: 2039  # GB/s
      processing_units: 108  # SMs
    - device_id: "gpu7"
      device_type: "GPU"
      peak_tflops:
        fp16: 312
        int8: 624
      memory_capacity_bytes: 80_000_000_000  # 80 GB
      memory_gbps: 2039  # GB/s
      processing_units: 108  # SMs
      
  network_links:
    # Client to server
    - link_id: "client_to_server"
      source_id: "client_node_0"
      dest_id: "framework_entry_0"
      bandwidth_bps: 10_000_000_000  # 10 Gbps
      latency_s: 0.0001  # 100 microseconds
      bidirectional: true
    # Inter-GPU links - create a mesh for 8 GPUs
    # Within TP groups (gpu0-3 for stage 0, gpu4-7 for stage 1)
    - link_id: "gpu0_to_gpu1"
      source_id: "gpu0"
      dest_id: "gpu1"
      bandwidth_bps: 600_000_000_000  # 600 Gbps (NVLink)
      latency_s: 0.000001  # 1 microsecond
      bidirectional: true
    - link_id: "gpu1_to_gpu2"
      source_id: "gpu1"
      dest_id: "gpu2"
      bandwidth_bps: 600_000_000_000
      latency_s: 0.000001
      bidirectional: true
    - link_id: "gpu2_to_gpu3"
      source_id: "gpu2"
      dest_id: "gpu3"
      bandwidth_bps: 600_000_000_000
      latency_s: 0.000001
      bidirectional: true
    - link_id: "gpu3_to_gpu0"
      source_id: "gpu3"
      dest_id: "gpu0"
      bandwidth_bps: 600_000_000_000
      latency_s: 0.000001
      bidirectional: true
    - link_id: "gpu4_to_gpu5"
      source_id: "gpu4"
      dest_id: "gpu5"
      bandwidth_bps: 600_000_000_000
      latency_s: 0.000001
      bidirectional: true
    - link_id: "gpu5_to_gpu6"
      source_id: "gpu5"
      dest_id: "gpu6"
      bandwidth_bps: 600_000_000_000
      latency_s: 0.000001
      bidirectional: true
    - link_id: "gpu6_to_gpu7"
      source_id: "gpu6"
      dest_id: "gpu7"
      bandwidth_bps: 600_000_000_000
      latency_s: 0.000001
      bidirectional: true
    - link_id: "gpu7_to_gpu4"
      source_id: "gpu7"
      dest_id: "gpu4"
      bandwidth_bps: 600_000_000_000
      latency_s: 0.000001
      bidirectional: true
    # Between PP stages
    - link_id: "gpu0_to_gpu4"
      source_id: "gpu0"
      dest_id: "gpu4"
      bandwidth_bps: 400_000_000_000  # 400 Gbps (cross-node)
      latency_s: 0.000002  # 2 microseconds
      bidirectional: true
    - link_id: "gpu1_to_gpu5"
      source_id: "gpu1"
      dest_id: "gpu5"
      bandwidth_bps: 400_000_000_000
      latency_s: 0.000002
      bidirectional: true
    - link_id: "gpu2_to_gpu6"
      source_id: "gpu2"
      dest_id: "gpu6"
      bandwidth_bps: 400_000_000_000
      latency_s: 0.000002
      bidirectional: true
    - link_id: "gpu3_to_gpu7"
      source_id: "gpu3"
      dest_id: "gpu7"
      bandwidth_bps: 400_000_000_000
      latency_s: 0.000002
      bidirectional: true

workload:
  total_duration: 600
  bytes_per_token_estimate_for_network: 2
  random_seed: 123
  max_turns_per_session: 1
  
  client_profiles:
    - profile_name: "test_load"
      weight: 1.0
      inter_arrival_time_dist_config:
        type: "Exponential"
        rate: 8.0  # 8 requests per second
      prompt_tokens_dist_config:
        type: "Normal"
        mean: 1024
        sigma: 128
        is_int: true
      max_output_tokens_dist_config:
        type: "Normal"
        mean: 512  # Longer sequences for PP benefit
        sigma: 64
        is_int: true
      turn_type_dist_config:
        type: "Fixed"
        value: "new_conversation"

frameworks_to_test:
  - name: "vllm_tp2pp2"
    type: "ParallelVLLM"
    is_target_for_workload: true
    config:
      model_profile_id: "GPT-3-175B"  # Very large model
      block_size: 16
      max_num_seqs: 256
      enable_prefix_caching: false
      enable_cross_request_caching: false
      enable_chunked_prefill: true
      prefill_chunk_size: 8192
      parallelism:
        strategy: "TP_PP"
        tp_degree: 4
        pp_stages: 2
        num_microbatches_per_request: 4
        gpu_ids: ["gpu0", "gpu1", "gpu2", "gpu3", "gpu4", "gpu5", "gpu6", "gpu7"]

metrics_config:
  output_summary_json_path: "./experiments/results/lod_test_tp2pp2_summary.json"
  output_requests_csv_path: "./experiments/results/lod_test_tp2pp2_requests.csv"
  compute_token_stats: true
  compute_percentiles:
    - 50
    - 90
    - 95
    - 99