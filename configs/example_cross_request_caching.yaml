# Example configuration for testing cross-request prefix caching

simulation:
  max_simulation_time: 30
  random_seed: 42

model_characteristics_db_path: "configs/model_params.json"

hardware_profile:
  compute_devices:
    - device_id: "gpu0"
      device_type: "GPU"
      peak_tflops:
        fp16: 312
        int8: 624
      memory_capacity_bytes: 80_000_000_000
      memory_gbps: 2039
      processing_units: 108
  
  network_links:
    - link_id: "client_to_server"
      source_id: "client_node_0"
      dest_id: "gpu0"
      bandwidth_bps: 10_000_000_000
      latency_s: 0.0001
      bidirectional: true

frameworks_to_test:
  - name: "vllm_cross_request"
    type: "VLLM"
    is_target_for_workload: true
    config:
      gpu_id: "gpu0"
      model_profile_id: "Llama2-7B"
      block_size: 16
      max_num_seqs: 32
      max_num_batched_tokens: 2048
      enable_prefix_caching: true
      enable_cross_request_caching: true
      min_prefix_cache_length: 50
      max_prefix_cache_size: 20
      prefix_eviction_policy: "lru"

workload:
  total_duration: 30
  bytes_per_token_estimate_for_network: 2
  generate_prompt_tokens: true  # Enable token generation
  prefix_patterns:
    patterns:
      # System prompt pattern (30% of requests)
      - type: "system"
        name: "helpful_assistant"
        weight: 0.3
      # Code assistant pattern (25% of requests)  
      - type: "system"
        name: "code_assistant"
        weight: 0.25
      # Few-shot classification (20% of requests)
      - type: "few_shot"
        name: "classification_3shot"
        weight: 0.2
      # Instruction templates (15% of requests)
      - type: "instruction"
        name: "analyze_data"
        weight: 0.15
      # Random (no prefix) (10% of requests)
      - type: "random"
        weight: 0.1
  
  client_profiles:
    # Profile 1: Uses system prompts frequently
    - profile_name: "system_prompt_user"
      weight: 0.4
      inter_arrival_time_dist_config:
        type: "Exponential"
        rate: 2.0  # 2 requests/second
      prompt_tokens_dist_config:
        type: "Uniform"
        low: 200
        high: 400
        is_int: true
      max_output_tokens_dist_config:
        type: "Uniform"
        low: 50
        high: 150
        is_int: true
      conversational_probability: 0.0  # No conversations for clearer cross-request testing
    
    # Profile 2: Uses few-shot examples
    - profile_name: "few_shot_user"
      weight: 0.3
      inter_arrival_time_dist_config:
        type: "Exponential"
        rate: 1.0  # 1 request/second
      prompt_tokens_dist_config:
        type: "Uniform"
        low: 600
        high: 800
        is_int: true
      max_output_tokens_dist_config:
        type: "Uniform"
        low: 100
        high: 200
        is_int: true
      conversational_probability: 0.0
    
    # Profile 3: Mixed patterns
    - profile_name: "mixed_user"
      weight: 0.3
      inter_arrival_time_dist_config:
        type: "Exponential"
        rate: 1.5  # 1.5 requests/second
      prompt_tokens_dist_config:
        type: "Uniform"
        low: 300
        high: 500
        is_int: true
      max_output_tokens_dist_config:
        type: "Uniform"
        low: 75
        high: 175
        is_int: true
      conversational_probability: 0.0

metrics_config:
  output_requests_csv_path: "experiments/results/cross_request_caching_requests.csv"
  output_summary_json_path: "experiments/results/cross_request_caching_summary.json"
  percentiles_to_calculate: [0.5, 0.9, 0.95, 0.99]
  warm_up_duration_s: 5  # Warm-up to populate cache